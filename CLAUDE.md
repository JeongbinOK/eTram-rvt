# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is the **eTraM (Event-based Traffic Monitoring Dataset)** repository, which contains implementations for event-based traffic monitoring using deep learning models. The project includes two main components:

1. **RVT (Recurrent Vision Transformers)** - Modified version for event-based object detection
2. **Ultralytics YOLO** - Modified version for event-based data

## Key Commands

### RVT Component (rvt_eTram/)

**Environment Setup:**
```bash
# Create environment using conda/mamba
conda env create -f environment.yaml
conda activate rvt
```

**Data Preprocessing:**
```bash
# Preprocess eTraM dataset to required format
python scripts/genx/preprocess_dataset.py <DATA_IN_PATH> <DATA_OUT_PATH> \
  conf_preprocess/representation/stacked_hist.yaml \
  conf_preprocess/extraction/const_duration.yaml \
  conf_preprocess/filter_gen4.yaml -ds gen4 -np <N_PROCESSES>
```

**Training:**
```bash
# Train RVT model
python train.py model=rnndet dataset=gen4 dataset.path=<DATA_DIR> \
  wandb.project_name=<WANDB_NAME> wandb.group_name=<WAND_GRP> \
  +experiment/gen4="default.yaml" hardware.gpus=0 batch_size.train=6 \
  batch_size.eval=2 hardware.num_workers.train=4 hardware.num_workers.eval=3 \
  training.max_epochs=20 dataset.train.sampling=stream +model.head.num_classes=3
```

**Evaluation:**
```bash
# Evaluate model
python validation.py dataset=gen4 dataset.path=${DATA_DIR} checkpoint=${CKPT_PATH} \
  use_test_set=${USE_TEST} hardware.gpus=${GPU_ID} +experiment/gen4="${MDL_CFG}.yaml" \
  batch_size.eval=8 model.postprocess.confidence_threshold=0.001
```

### Ultralytics Component (ultralytics_eTram/)

**Environment Setup:**
```bash
# Install requirements
pip install -r requirements.txt
```

**Training/Inference:**
```bash
# Run YOLO training/inference
cd yolo_eTram
python main.py
```

## Architecture Overview

### RVT Architecture (rvt_eTram/)

- **Configuration System**: Uses Hydra for configuration management with YAML files in `config/`
- **Main Components**:
  - `train.py`: Main training script with PyTorch Lightning
  - `modules/detection.py`: Core detection module with RNN states management
  - `models/detection/`: Model architectures including YOLOX detector and recurrent backbones
  - `data/`: Data loading and preprocessing utilities
  - `callbacks/`: Custom callbacks for visualization and monitoring

- **Key Features**:
  - Supports streaming and random sampling modes
  - Uses W&B for experiment tracking
  - Implements confusion matrix evaluation
  - Multi-GPU training support with DDP strategy

### Model Configuration

The project uses hierarchical YAML configuration:
- `config/model/`: Model architectures (base, small, tiny variants)
- `config/dataset/`: Dataset configurations (gen1, gen4, etrap)
- `config/experiment/`: Experiment-specific configs combining model and dataset settings

### Data Pipeline

- **Event Representation**: Uses stacked histogram representation
- **Preprocessing**: Constant duration extraction with filtering
- **Classes**: 8 traffic participant classes (vehicles, pedestrians, micro-mobility)
- **Evaluation**: Uses Prophesee evaluation metrics and confusion matrix

## Development Guidelines

### Working with Configurations

- Model configurations are in `config/model/`
- Dataset paths and parameters are in `config/dataset/`
- Experiment configs combine model and dataset settings
- Use `+experiment/gen4="config_name.yaml"` to load experiment configs

### Training Workflow

1. Preprocess dataset using `preprocess_dataset.py`
2. Configure model, dataset, and experiment parameters
3. Run training with appropriate GPU and batch size settings
4. Monitor training through W&B (if enabled)
5. Evaluate using `validation.py`

### Model Variants

- **RVT-base**: Full model with maximum performance
- **RVT-small**: Reduced model size for faster training
- **RVT-tiny**: Minimal model for resource-constrained environments

### Data Handling

- Uses PyTorch Lightning data modules
- Supports both streaming and random sampling
- Implements custom data loaders for event-based data
- Includes data augmentation and padding utilities

## RVT Model Architecture Summary

### High-Level Architecture Overview

```
Event Data â†’ Recurrent Backbone â†’ Feature Pyramid â†’ Detection Head â†’ Predictions
    â†“              â†“                    â†“              â†“            â†“
Preprocessing   MaxViT+LSTM         YOLO PAFPN    YOLOX Head    Post-process
```

### File Structure & Component Mapping

```
rvt_eTram/
â”œâ”€â”€ models/detection/
â”‚   â”œâ”€â”€ yolox_extension/models/
â”‚   â”‚   â”œâ”€â”€ detector.py           # Main YoloXDetector class
â”‚   â”‚   â”œâ”€â”€ build.py             # Component builders
â”‚   â”‚   â””â”€â”€ yolo_pafpn.py        # Feature Pyramid Network
â”‚   â”œâ”€â”€ recurrent_backbone/
â”‚   â”‚   â”œâ”€â”€ base.py              # BaseDetector interface
â”‚   â”‚   â””â”€â”€ maxvit_rnn.py        # RNN Backbone (Core)
â”‚   â””â”€â”€ yolox/models/
â”‚       â”œâ”€â”€ yolo_head.py         # Detection head
â”‚       â”œâ”€â”€ losses.py            # Loss functions
â”‚       â””â”€â”€ network_blocks.py    # Basic building blocks
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ detection.py             # PyTorch Lightning module
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model/                   # Model configurations
â”‚   â””â”€â”€ experiment/              # Experiment configs
â””â”€â”€ train.py                     # Main training script
```

### Visual Architecture Diagram

```
INPUT: Event Representation (N, C, H, W)
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECURRENT BACKBONE                           â”‚
â”‚              maxvit_rnn.py (RNNDetector)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Stage 1: Patch=4  â”‚ Stage 2: â†“2    â”‚ Stage 3: â†“2    â”‚ Stage 4: â†“2   â”‚
â”‚  Stride: 4         â”‚ Stride: 8      â”‚ Stride: 16     â”‚ Stride: 32    â”‚
â”‚  Dim: 64           â”‚ Dim: 128       â”‚ Dim: 256       â”‚ Dim: 512      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Downsample  â”‚   â”‚ â”‚ Downsample  â”‚ â”‚ â”‚ Downsample  â”‚ â”‚ â”‚ Downsample  â”‚ â”‚
â”‚  â”‚     â†“       â”‚   â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚
â”‚  â”‚ MaxViT      â”‚   â”‚ â”‚ MaxViT      â”‚ â”‚ â”‚ MaxViT      â”‚ â”‚ â”‚ MaxViT      â”‚ â”‚
â”‚  â”‚ Attention   â”‚   â”‚ â”‚ Attention   â”‚ â”‚ â”‚ Attention   â”‚ â”‚ â”‚ Attention   â”‚ â”‚
â”‚  â”‚ (Window+Grid)â”‚  â”‚ â”‚ (Window+Grid)â”‚ â”‚ â”‚ (Window+Grid)â”‚ â”‚ â”‚ (Window+Grid)â”‚ â”‚
â”‚  â”‚     â†“       â”‚   â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚
â”‚  â”‚ ConvLSTM    â”‚   â”‚ â”‚ ConvLSTM    â”‚ â”‚ â”‚ ConvLSTM    â”‚ â”‚ â”‚ ConvLSTM    â”‚ â”‚
â”‚  â”‚ (Recurrent) â”‚   â”‚ â”‚ (Recurrent) â”‚ â”‚ â”‚ (Recurrent) â”‚ â”‚ â”‚ (Recurrent) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                    â”‚                â”‚                â”‚                â”‚
â”‚     P1: H/4Ã—W/4    â”‚   P2: H/8Ã—W/8  â”‚  P3: H/16Ã—W/16 â”‚  P4: H/32Ã—W/32 â”‚
â”‚     (not used)     â”‚   (for FPN)    â”‚   (for FPN)    â”‚   (for FPN)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                â”‚                â”‚
          â–¼                    â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FEATURE PYRAMID NETWORK                        â”‚
â”‚                 yolo_pafpn.py (YOLOPAFPN)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  P4 (512) â”€â”€1Ã—1â”€â”€â†’ 256 â”€â”€â†‘Ã—2â”€â”€â”                               â”‚
â”‚                              â”‚                                â”‚
â”‚  P3 (256) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ âŠ• â”€â”€CSPâ”€â”€â†’ 256 â”€â”€1Ã—1â”€â”€â†’ 128 â”€â”€â†‘Ã—2â”€â”€â” â”‚
â”‚                                                               â”‚ â”‚
â”‚  P2 (128) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ âŠ• â”€â”€CSPâ”€â”€â†’ N3 (128) â”‚
â”‚                                                               â”‚         â”‚
â”‚                                              â”Œâ”€â”€â”€â”€3Ã—3â†“2â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                              â”‚                         â”‚
â”‚  N4 (256) â†â”€â”€CSPâ†â”€â”€âŠ•â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚              â”‚                                                       â”‚
â”‚              â””â”€â”€â”€â”€â”€3Ã—3â†“2â”€â”€â”€â”€â†’ âŠ• â”€â”€â”€â”€CSPâ”€â”€â”€â†’ N5 (512)                 â”‚
â”‚                               â”‚                                       â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â† P4                       â”‚
â”‚                                                                       â”‚
â”‚  OUTPUT: (N3: 128@H/8, N4: 256@H/16, N5: 512@H/32)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                â”‚
          â–¼                    â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DETECTION HEAD                              â”‚
â”‚                 yolo_head.py (YOLOXHead)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  For each scale (stride 8, 16, 32):                           â”‚
â”‚                                                                 â”‚
â”‚  Feature â”€â”€1Ã—1â”€â”€â†’ Hidden (256) â”€â”€â”¬â”€â”€â†’ Cls Conv â”€â”€â†’ Cls Pred    â”‚
â”‚                                  â”‚    (3Ã—3Ã—2)      (1Ã—1)       â”‚
â”‚                                  â”‚                             â”‚
â”‚                                  â””â”€â”€â†’ Reg Conv â”€â”€â”¬â”€â”€â†’ Reg Pred â”‚
â”‚                                       (3Ã—3Ã—2)    â”‚    (1Ã—1Ã—4)  â”‚
â”‚                                                  â”‚             â”‚
â”‚                                                  â””â”€â”€â†’ Obj Pred â”‚
â”‚                                                       (1Ã—1Ã—1)  â”‚
â”‚                                                                 â”‚
â”‚  Output: [Reg(4) + Obj(1) + Cls(num_classes)] per anchor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   POST-PROCESSING                              â”‚
â”‚                boxes.py (postprocess)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Confidence Thresholding                                    â”‚
â”‚  2. NMS (Non-Maximum Suppression)                              â”‚
â”‚  3. Coordinate Decoding                                        â”‚
â”‚  4. Final Detections                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### 1. Main Detector (`detector.py:18`)
```python
class YoloXDetector(th.nn.Module):
    def __init__(self, model_cfg):
        self.backbone = build_recurrent_backbone(backbone_cfg)
        self.fpn = build_yolox_fpn(fpn_cfg, in_channels)
        self.yolox_head = build_yolox_head(head_cfg, in_channels, strides)
```

#### 2. Recurrent Backbone (`maxvit_rnn.py:23`)
```python
class RNNDetector(BaseDetector):
    # 4-stage hierarchical backbone
    # Each stage: Downsample â†’ MaxViT Attention â†’ ConvLSTM
    # Strides: [4, 8, 16, 32]
    # Dims: [64, 128, 256, 512] (configurable)
```

#### 3. Feature Pyramid (`yolo_pafpn.py:18`)
```python
class YOLOPAFPN(nn.Module):
    # Top-down + Bottom-up feature fusion
    # Input: P2, P3, P4 from backbone
    # Output: N3, N4, N5 for detection
```

#### 4. Detection Head (`yolo_head.py:21`)
```python
class YOLOXHead(nn.Module):
    # Per-scale processing: 3 scales (8, 16, 32)
    # Outputs: Classification + Regression + Objectness
    # Uses anchor-free detection with center-based assignment
```

### Data Flow Summary

1. **Input**: Event representation (stacked histograms) - `(B, C, H, W)`
2. **Backbone**: 4-stage processing with recurrent memory - `{P1, P2, P3, P4}`
3. **FPN**: Multi-scale feature fusion - `{N3, N4, N5}`
4. **Detection**: Per-scale predictions - `[reg, obj, cls]`
5. **Output**: Bounding boxes with confidence scores

### Key Configuration Files

- **Model Config**: `config/model/maxvit_yolox/default.yaml`
- **Training Config**: `config/experiment/gen4/default.yaml`
- **Dataset Config**: `config/dataset/gen4.yaml`

### Memory and Recurrence

The key innovation of RVT is the **recurrent processing** at each backbone stage:

```python
# From maxvit_rnn.py:180
h_c_tuple = self.lstm(x, h_and_c_previous)  # ConvLSTM with memory
```

Each stage maintains **hidden states** across time steps, enabling temporal modeling of event sequences. This is managed by:

- **RNNStates** (`modules/detection.py:22`): Manages LSTM states per training mode
- **LstmStates** type: List of hidden/cell state tuples for each backbone stage

### âœ… PHASE 1 COMPLETED - P1 Feature Integration (2025-07-04)

**Successfully implemented 4-scale FPN for small object detection!**

**Modified Files:**
1. **`rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py`**:
   - Extended YOLOPAFPN class to support both 3-scale and 4-scale configurations
   - Added adaptive layer creation based on `num_scales` parameter
   - Implemented 4-scale top-down pathway: P4â†’P3â†’P2â†’P1
   - Implemented 4-scale bottom-up pathway: N1â†’N2â†’N3â†’N4
   - Maintains backward compatibility with original 3-scale mode

2. **`rvt_eTram/config/model/maxvit_yolox/default.yaml`**:
   - Changed `in_stages: [2, 3, 4]` â†’ `in_stages: [1, 2, 3, 4]`
   - Enables P1 features (stride 4) for small object detection

**Current Architecture Status:**
- âœ… **P1 features now utilized** (stride 4) - highest resolution features active
- âœ… **FPN supports 4 scales** (stride 4, 8, 16, 32) - enhanced fine detail
- âœ… **Detection head automatically supports 4 scales** - no changes needed
- ğŸ”„ **Anchor assignment** still optimized for medium/large objects (Phase 2)
- ğŸ”„ **Loss weighting** doesn't prioritize small object scales (Phase 2)

## Small Object Detection Enhancement Plan

### Strategy Overview

Event-based small object detection faces unique challenges:
- **Sparse Event Generation**: Small objects generate fewer events
- **Temporal Inconsistency**: Irregular movement patterns
- **Signal-to-Noise Ratio**: Difficulty distinguishing small objects from noise
- **Resolution Loss**: Current architecture discards high-resolution features

### âœ… Phase 1: High-Resolution Feature Integration (COMPLETED)

**Implementation Status: DONE âœ…**

#### 1.1 Backbone Modification âœ…
- **Status**: No changes needed - backbone already outputs all stages
- **Verification**: `maxvit_rnn.py:104` already returns `{1: P1, 2: P2, 3: P3, 4: P4}`

#### 1.2 FPN Extension âœ… 
- **File**: `rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py`
- **Changes**:
  - Extended `__init__` to support 4-scale: `in_stages=[1,2,3,4], in_channels=[64,128,256,512]`
  - Added conditional layer creation for 4-scale vs 3-scale
  - Implemented P1 processing layers: `reduce_conv2`, `C3_p2`, `bu_conv3`, `C3_n2`
  - Updated `forward()` method for 4-scale pathway

#### 1.3 Detection Head âœ…
- **Status**: No changes needed - YOLOXHead automatically supports variable scales
- **Verification**: Tested with 4 input scales, works correctly

**Testing Results:**
```
âœ“ 4-scale FPN initialization successful
âœ“ Forward pass successful - Output shapes:
  N1: torch.Size([2, 64, 80, 80]) (stride 4)
  N2: torch.Size([2, 128, 40, 40]) (stride 8)  
  N3: torch.Size([2, 256, 20, 20]) (stride 16)
  N4: torch.Size([2, 512, 10, 10]) (stride 32)
âœ“ Total detection scales: 4 (including stride 4 for small objects)
```

### Phase 2: Small Object Specialized Components (Medium Priority)

#### 2.1 Small Object Attention Module
```python
class SmallObjectAttention(nn.Module):
    def __init__(self, channels, scale_factor):
        self.motion_attention = MotionAwareAttention()
        self.spatial_attention = SpatialAttention()  
        self.scale_attention = ScaleAwareAttention(scale_factor)
```

#### 2.2 Temporal Feature Enhancement
```python
class TemporalFeatureEnhancer(nn.Module):
    def __init__(self):
        self.multi_temporal_fusion = MultiTemporalFusion()
        self.temporal_consistency = TemporalConsistency()
```

#### 2.3 Size-Aware Loss Function
```python
class SizeAwareLoss(nn.Module):
    def forward(self, pred, target, bbox_sizes):
        # Higher weight for smaller objects
        size_weights = torch.exp(-bbox_sizes / threshold)
        weighted_loss = base_loss * size_weights
```

### Phase 3: Training Strategy Optimization

#### 3.1 Adaptive Sampling
- Increase sampling ratio for small object samples
- Hard negative mining for difficult small object cases

#### 3.2 Multi-Scale Training  
- Various input resolutions during training
- Scale jittering for robustness improvement

### Implementation Priority

**High Priority (Immediate Implementation):**
1. P1 Feature Integration (Backbone + FPN + Head)
2. Size-Aware Loss Function
3. Training Strategy Improvements

**Medium Priority (Secondary Implementation):**
4. Small Object Attention Mechanisms
5. Temporal Enhancement Modules
6. Advanced Event Representations

**Low Priority (Experimental):**
7. Deformable Convolutions
8. Neural Architecture Search adaptations

### Expected Performance Improvements

- **Small Object AP**: +15-25% improvement expected
- **Overall mAP**: +5-10% improvement expected  
- **Temporal Consistency**: Significant improvement in tracking
- **False Positive Rate**: Reduction through better noise distinction

### File Modification Summary

**Core Files to Modify:**
- `rvt_eTram/models/detection/recurrent_backbone/maxvit_rnn.py` - Add P1 output
- `rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py` - 4-scale FPN
- `rvt_eTram/models/detection/yolox/models/yolo_head.py` - 4-scale detection
- `rvt_eTram/models/detection/yolox/models/losses.py` - Size-aware loss
- `rvt_eTram/config/model/maxvit_yolox/default.yaml` - Configuration updates

**âœ… Configuration Changes Applied:**
```yaml
# rvt_eTram/config/model/maxvit_yolox/default.yaml
fpn:
  in_stages: [1, 2, 3, 4]  # âœ… IMPLEMENTED - Include P1 for small objects
  # Note: in_channels automatically inferred from backbone: [64, 128, 256, 512]

# Detection head automatically inherits strides: [4, 8, 16, 32] âœ… WORKING
```

**ğŸš€ Ready for Training:**
The implementation is complete and tested. You can now run training with enhanced small object detection:

```bash
# Train with P1 features enabled
python train.py model=rnndet dataset=gen4 dataset.path=<DATA_DIR> \
  +experiment/gen4="default.yaml" hardware.gpus=0 batch_size.train=6 \
  batch_size.eval=2 training.max_epochs=20
```

This systematic approach will significantly enhance small object detection performance while maintaining the RVT architecture's temporal modeling strengths.

## Small Object Detection Enhancement Experiments

### ğŸ¯ ì‹¤í—˜ ëª©í‘œ ë° ì„¤ì •

**ë°ì´í„°ì…‹ (ê³ ì •):**
- **ë©”ì¸ ë°ì´í„°ì…‹**: `etram_cls8_sample`
- **ê²½ë¡œ**: `/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample`
- **í´ë˜ìŠ¤ ìˆ˜**: 8ê°œ (Car, Truck, Motorcycle, Bicycle, Pedestrian, Bus, Static, Other)
- **ëª©í‘œ**: Small object detection ì„±ëŠ¥ í–¥ìƒ (í´ë˜ìŠ¤ 2,3,4: Motorcycle, Bicycle, Pedestrian)

**í•„ìˆ˜ ì„¤ì •ê°’ (ë¡œì»¬ ë©”ëª¨ë¦¬):**
- **í´ë˜ìŠ¤ ìˆ˜**: `+model.head.num_classes=8` (í•„ìˆ˜!)
- **í›ˆë ¨ ìŠ¤í…**: `training.max_steps=100000` (í•„ìˆ˜!)
- **Screen ì‚¬ìš©**: ëª¨ë“  í›ˆë ¨/validationì—ì„œ í•„ìˆ˜
- **ë°ì´í„°ì…‹**: `dataset=gen4`
- **ë°ì´í„° ê²½ë¡œ**: `dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample`

### ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (3-scale FPN)

**ì „ì²´ ì„±ëŠ¥:**
- **Overall mAP**: 34.02%
- **AP50**: 67.03%
- **AP75**: 30.79%

**í¬ê¸°ë³„ ì„±ëŠ¥:**
- **ğŸ”´ Small objects**: 17.28% mAP (í´ë˜ìŠ¤ 2,3,4: Motorcycle, Bicycle, Pedestrian) âš ï¸ **ì£¼ìš” ê°œì„  íƒ€ê²Ÿ**
- **ğŸŸ¡ Medium objects**: 34.03% mAP (í´ë˜ìŠ¤ 0,1,5,6,7: Car, Truck, Bus, Static, Other)
- **ğŸŸ¢ Large objects**: 56.94% mAP (ë§¤ìš° í° ê°ì²´ë“¤)

### ğŸ“‹ ì‹¤í—˜ í•œ ì‚¬ì´í´ í‘œì¤€ í”„ë¡œì„¸ìŠ¤

#### Phase 1: ì‹¤í—˜ ì„¤ì • ë° ì¤€ë¹„ (10ë¶„)

```bash
# 1. ì‹¤í—˜ í´ë” ìƒì„±
EXPERIMENT_ID="4scale_enhanced_100k"  # í˜•ì‹: {architecture}_{modification}_{steps}
mkdir -p experiments/${EXPERIMENT_ID}/{checkpoints,confusion_matrices,training_logs,validation_results}

# 2. ëª¨ë¸ ì„¤ì • ë³€ê²½
# íŒŒì¼: config/model/maxvit_yolox/default.yaml
# 3-scale: in_stages: [2, 3, 4]
# 4-scale: in_stages: [1, 2, 3, 4]  # P1 features í™œì„±í™”

# 3. ì„¤ì • ë°±ì—…
cp config/model/maxvit_yolox/default.yaml experiments/${EXPERIMENT_ID}/model_config.yaml
```

#### Phase 2: í›ˆë ¨ ì‹¤í–‰ (5-6ì‹œê°„)

```bash
# Screenì—ì„œ í›ˆë ¨ (í•„ìˆ˜!)
screen -dmS ${EXPERIMENT_ID}
screen -S ${EXPERIMENT_ID} -p 0 -X stuff "cd /home/oeoiewt/eTraM/rvt_eTram\n"
screen -S ${EXPERIMENT_ID} -p 0 -X stuff "python train.py model=rnndet dataset=gen4 dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample +experiment/gen4='default.yaml' hardware.gpus=0 batch_size.train=6 batch_size.eval=2 hardware.num_workers.train=4 hardware.num_workers.eval=3 training.max_steps=100000 dataset.train.sampling=stream +model.head.num_classes=8 wandb.project_name=etram_enhanced wandb.group_name=${EXPERIMENT_ID}; echo 'Training completed! Press Enter to continue...'; read\n"
```

#### Phase 3: ê²°ê³¼ ìˆ˜ì§‘ ë° ì •ë¦¬ (30ë¶„)

```bash
# 1. ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
cp dummy/${WANDB_ID}/checkpoints/epoch=*-step=100000-*.ckpt experiments/${EXPERIMENT_ID}/checkpoints/final_model.ckpt

# 2. Confusion Matrix ì´ë™
mv confM/* experiments/${EXPERIMENT_ID}/confusion_matrices/

# 3. Screen ì„¸ì…˜ ì •ë¦¬
screen -r ${EXPERIMENT_ID}  # ì™„ë£Œ í™•ì¸
```

#### Phase 4: Validation ë° ìƒì„¸ ì§€í‘œ (10ë¶„)

```bash
# Screenì—ì„œ Validation ì‹¤í–‰ (í•„ìˆ˜!)
screen -dmS validation_${EXPERIMENT_ID}
screen -S validation_${EXPERIMENT_ID} -p 0 -X stuff "cd /home/oeoiewt/eTraM/rvt_eTram\n"
screen -S validation_${EXPERIMENT_ID} -p 0 -X stuff "python validation.py dataset=gen4 dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample checkpoint=experiments/${EXPERIMENT_ID}/checkpoints/final_model.ckpt +experiment/gen4='default.yaml' hardware.gpus=0 batch_size.eval=8 +model.head.num_classes=8; echo 'Validation completed! Press Enter to continue...'; read\n"

# ê²°ê³¼ ì €ì¥
mkdir -p validation_results/${EXPERIMENT_ID}
screen -r validation_${EXPERIMENT_ID} -X hardcopy /tmp/validation_${EXPERIMENT_ID}.txt
cp /tmp/validation_${EXPERIMENT_ID}.txt validation_results/${EXPERIMENT_ID}/validation_output.log
```

#### Phase 5: ê²°ê³¼ ë¶„ì„ ë° ë¬¸ì„œí™” (20ë¶„)

```bash
# 1. ì„±ëŠ¥ ìš”ì•½ íŒŒì¼ ìƒì„±
# validation_results/${EXPERIMENT_ID}/metrics_summary.txt
# validation_results/${EXPERIMENT_ID}/evaluation_info.txt

# 2. ì‹¤í—˜ ê²°ê³¼ JSON ìƒì„±
# experiments/${EXPERIMENT_ID}/experiment_results.json
```

#### Phase 6: Git ê´€ë¦¬ ë° ë³´ì¡´ (10ë¶„)

```bash
# ì‹¤í—˜ ê²°ê³¼ ì»¤ë°‹
git add experiments/${EXPERIMENT_ID}/
git add validation_results/${EXPERIMENT_ID}/
git commit -m "feat: complete ${EXPERIMENT_ID} experiment

- Model: [êµ¬ì²´ì  ì•„í‚¤í…ì²˜ ì„¤ëª…]
- Performance: mAP X.XX% (+X.X% vs baseline)
- Small objects: X.XX% mAP (+X.X% improvement)
- Key findings: [ì£¼ìš” ë°œê²¬ì‚¬í•­]

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)
Co-Authored-By: Claude <noreply@anthropic.com>"

# Screen ì„¸ì…˜ ì •ë¦¬
screen -S ${EXPERIMENT_ID} -X quit
screen -S validation_${EXPERIMENT_ID} -X quit
```

### ğŸ¯ ì„±ëŠ¥ ê°œì„  ëª©í‘œ

**Small Object Detection ëª©í‘œ:**
- **í˜„ì¬ ë² ì´ìŠ¤ë¼ì¸**: 17.28% mAP (Small objects)
- **4-scale FPN ëª©í‘œ**: 20-22% mAP (+15-25% í–¥ìƒ)
- **ì „ì²´ ì„±ëŠ¥ ëª©í‘œ**: 36-38% mAP (+5-10% í–¥ìƒ)

**ì‹¤í—˜ ì‹œë¦¬ì¦ˆ:**
1. âœ… **3-scale Baseline** (ì™„ë£Œ): 34.02% mAP
2. ğŸ”„ **4-scale Enhanced**: P1 features ì¶”ê°€ (ì§„í–‰ ì¤‘)
3. ğŸ”® **Size-aware Loss**: Loss function ê°œì„ 
4. ğŸ”® **Attention Modules**: Small object ì „ìš© attention

### ğŸ“ í‘œì¤€ íŒŒì¼ êµ¬ì¡°

```
experiments/{EXPERIMENT_ID}/
â”œâ”€â”€ checkpoints/final_model.ckpt
â”œâ”€â”€ confusion_matrices/*.png
â”œâ”€â”€ model_config.yaml
â”œâ”€â”€ training_logs/
â”œâ”€â”€ validation_results/ â†’ validation_results/{EXPERIMENT_ID}/
â””â”€â”€ experiment_results.json

validation_results/{EXPERIMENT_ID}/
â”œâ”€â”€ validation_output.log
â”œâ”€â”€ metrics_summary.txt
â””â”€â”€ evaluation_info.txt
```

### ğŸ”¬ ì‹¤í—˜ ê´€ë¦¬ ì›ì¹™

1. **ì¬í˜„ì„±**: ëª¨ë“  ì„¤ì •ì„ Gitìœ¼ë¡œ ê´€ë¦¬
2. **ì²´ê³„ì„±**: í‘œì¤€ í´ë” êµ¬ì¡° ìœ ì§€
3. **ë¹„êµì„±**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì •
4. **ë¬¸ì„œí™”**: ê° ì‹¤í—˜ì˜ ëª©ì ê³¼ ê²°ê³¼ ëª…í™•íˆ ê¸°ë¡