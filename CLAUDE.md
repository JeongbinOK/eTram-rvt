# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is the **eTraM (Event-based Traffic Monitoring Dataset)** repository, which contains implementations for event-based traffic monitoring using deep learning models. The project includes two main components:

1. **RVT (Recurrent Vision Transformers)** - Modified version for event-based object detection
2. **Ultralytics YOLO** - Modified version for event-based data

## Key Commands

### RVT Component (rvt_eTram/)

**Environment Setup:**
```bash
# Create environment using conda/mamba
conda env create -f environment.yaml
conda activate rvt
```

**Data Preprocessing:**
```bash
# Preprocess eTraM dataset to required format
python scripts/genx/preprocess_dataset.py <DATA_IN_PATH> <DATA_OUT_PATH> \
  conf_preprocess/representation/stacked_hist.yaml \
  conf_preprocess/extraction/const_duration.yaml \
  conf_preprocess/filter_gen4.yaml -ds gen4 -np <N_PROCESSES>
```

**Training:**
```bash
# Train RVT model
python train.py model=rnndet dataset=gen4 dataset.path=<DATA_DIR> \
  wandb.project_name=<WANDB_NAME> wandb.group_name=<WAND_GRP> \
  +experiment/gen4="default.yaml" hardware.gpus=0 batch_size.train=6 \
  batch_size.eval=2 hardware.num_workers.train=4 hardware.num_workers.eval=3 \
  training.max_epochs=20 dataset.train.sampling=stream +model.head.num_classes=3
```

**Evaluation:**
```bash
# Evaluate model
python validation.py dataset=gen4 dataset.path=${DATA_DIR} checkpoint=${CKPT_PATH} \
  use_test_set=${USE_TEST} hardware.gpus=${GPU_ID} +experiment/gen4="${MDL_CFG}.yaml" \
  batch_size.eval=8 model.postprocess.confidence_threshold=0.001
```

### Ultralytics Component (ultralytics_eTram/)

**Environment Setup:**
```bash
# Install requirements
pip install -r requirements.txt
```

**Training/Inference:**
```bash
# Run YOLO training/inference
cd yolo_eTram
python main.py
```

## Architecture Overview

### RVT Architecture (rvt_eTram/)

- **Configuration System**: Uses Hydra for configuration management with YAML files in `config/`
- **Main Components**:
  - `train.py`: Main training script with PyTorch Lightning
  - `modules/detection.py`: Core detection module with RNN states management
  - `models/detection/`: Model architectures including YOLOX detector and recurrent backbones
  - `data/`: Data loading and preprocessing utilities
  - `callbacks/`: Custom callbacks for visualization and monitoring

- **Key Features**:
  - Supports streaming and random sampling modes
  - Uses W&B for experiment tracking
  - Implements confusion matrix evaluation
  - Multi-GPU training support with DDP strategy

### Model Configuration

The project uses hierarchical YAML configuration:
- `config/model/`: Model architectures (base, small, tiny variants)
- `config/dataset/`: Dataset configurations (gen1, gen4, etrap)
- `config/experiment/`: Experiment-specific configs combining model and dataset settings

### Data Pipeline

- **Event Representation**: Uses stacked histogram representation
- **Preprocessing**: Constant duration extraction with filtering
- **Classes**: 8 traffic participant classes (vehicles, pedestrians, micro-mobility)
- **Evaluation**: Uses Prophesee evaluation metrics and confusion matrix

## Development Guidelines

### Working with Configurations

- Model configurations are in `config/model/`
- Dataset paths and parameters are in `config/dataset/`
- Experiment configs combine model and dataset settings
- Use `+experiment/gen4="config_name.yaml"` to load experiment configs

### Training Workflow

1. Preprocess dataset using `preprocess_dataset.py`
2. Configure model, dataset, and experiment parameters
3. Run training with appropriate GPU and batch size settings
4. Monitor training through W&B (if enabled)
5. Evaluate using `validation.py`

### Model Variants

- **RVT-base**: Full model with maximum performance
- **RVT-small**: Reduced model size for faster training
- **RVT-tiny**: Minimal model for resource-constrained environments

### Data Handling

- Uses PyTorch Lightning data modules
- Supports both streaming and random sampling
- Implements custom data loaders for event-based data
- Includes data augmentation and padding utilities

## RVT Model Architecture Summary

### High-Level Architecture Overview

```
Event Data â†’ Recurrent Backbone â†’ Feature Pyramid â†’ Detection Head â†’ Predictions
    â†“              â†“                    â†“              â†“            â†“
Preprocessing   MaxViT+LSTM         YOLO PAFPN    YOLOX Head    Post-process
```

### File Structure & Component Mapping

```
rvt_eTram/
â”œâ”€â”€ models/detection/
â”‚   â”œâ”€â”€ yolox_extension/models/
â”‚   â”‚   â”œâ”€â”€ detector.py           # Main YoloXDetector class
â”‚   â”‚   â”œâ”€â”€ build.py             # Component builders
â”‚   â”‚   â””â”€â”€ yolo_pafpn.py        # Feature Pyramid Network
â”‚   â”œâ”€â”€ recurrent_backbone/
â”‚   â”‚   â”œâ”€â”€ base.py              # BaseDetector interface
â”‚   â”‚   â””â”€â”€ maxvit_rnn.py        # RNN Backbone (Core)
â”‚   â””â”€â”€ yolox/models/
â”‚       â”œâ”€â”€ yolo_head.py         # Detection head
â”‚       â”œâ”€â”€ losses.py            # Loss functions
â”‚       â””â”€â”€ network_blocks.py    # Basic building blocks
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ detection.py             # PyTorch Lightning module
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model/                   # Model configurations
â”‚   â””â”€â”€ experiment/              # Experiment configs
â””â”€â”€ train.py                     # Main training script
```

### Visual Architecture Diagram

```
INPUT: Event Representation (N, C, H, W)
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECURRENT BACKBONE                           â”‚
â”‚              maxvit_rnn.py (RNNDetector)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Stage 1: Patch=4  â”‚ Stage 2: â†“2    â”‚ Stage 3: â†“2    â”‚ Stage 4: â†“2   â”‚
â”‚  Stride: 4         â”‚ Stride: 8      â”‚ Stride: 16     â”‚ Stride: 32    â”‚
â”‚  Dim: 64           â”‚ Dim: 128       â”‚ Dim: 256       â”‚ Dim: 512      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Downsample  â”‚   â”‚ â”‚ Downsample  â”‚ â”‚ â”‚ Downsample  â”‚ â”‚ â”‚ Downsample  â”‚ â”‚
â”‚  â”‚     â†“       â”‚   â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚
â”‚  â”‚ MaxViT      â”‚   â”‚ â”‚ MaxViT      â”‚ â”‚ â”‚ MaxViT      â”‚ â”‚ â”‚ MaxViT      â”‚ â”‚
â”‚  â”‚ Attention   â”‚   â”‚ â”‚ Attention   â”‚ â”‚ â”‚ Attention   â”‚ â”‚ â”‚ Attention   â”‚ â”‚
â”‚  â”‚ (Window+Grid)â”‚  â”‚ â”‚ (Window+Grid)â”‚ â”‚ â”‚ (Window+Grid)â”‚ â”‚ â”‚ (Window+Grid)â”‚ â”‚
â”‚  â”‚     â†“       â”‚   â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚ â”‚     â†“       â”‚ â”‚
â”‚  â”‚ ConvLSTM    â”‚   â”‚ â”‚ ConvLSTM    â”‚ â”‚ â”‚ ConvLSTM    â”‚ â”‚ â”‚ ConvLSTM    â”‚ â”‚
â”‚  â”‚ (Recurrent) â”‚   â”‚ â”‚ (Recurrent) â”‚ â”‚ â”‚ (Recurrent) â”‚ â”‚ â”‚ (Recurrent) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                    â”‚                â”‚                â”‚                â”‚
â”‚     P1: H/4Ã—W/4    â”‚   P2: H/8Ã—W/8  â”‚  P3: H/16Ã—W/16 â”‚  P4: H/32Ã—W/32 â”‚
â”‚     (not used)     â”‚   (for FPN)    â”‚   (for FPN)    â”‚   (for FPN)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                â”‚                â”‚
          â–¼                    â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FEATURE PYRAMID NETWORK                        â”‚
â”‚                 yolo_pafpn.py (YOLOPAFPN)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  P4 (512) â”€â”€1Ã—1â”€â”€â†’ 256 â”€â”€â†‘Ã—2â”€â”€â”                               â”‚
â”‚                              â”‚                                â”‚
â”‚  P3 (256) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ âŠ• â”€â”€CSPâ”€â”€â†’ 256 â”€â”€1Ã—1â”€â”€â†’ 128 â”€â”€â†‘Ã—2â”€â”€â” â”‚
â”‚                                                               â”‚ â”‚
â”‚  P2 (128) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ âŠ• â”€â”€CSPâ”€â”€â†’ N3 (128) â”‚
â”‚                                                               â”‚         â”‚
â”‚                                              â”Œâ”€â”€â”€â”€3Ã—3â†“2â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                              â”‚                         â”‚
â”‚  N4 (256) â†â”€â”€CSPâ†â”€â”€âŠ•â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚              â”‚                                                       â”‚
â”‚              â””â”€â”€â”€â”€â”€3Ã—3â†“2â”€â”€â”€â”€â†’ âŠ• â”€â”€â”€â”€CSPâ”€â”€â”€â†’ N5 (512)                 â”‚
â”‚                               â”‚                                       â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â† P4                       â”‚
â”‚                                                                       â”‚
â”‚  OUTPUT: (N3: 128@H/8, N4: 256@H/16, N5: 512@H/32)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                â”‚
          â–¼                    â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DETECTION HEAD                              â”‚
â”‚                 yolo_head.py (YOLOXHead)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  For each scale (stride 8, 16, 32):                           â”‚
â”‚                                                                 â”‚
â”‚  Feature â”€â”€1Ã—1â”€â”€â†’ Hidden (256) â”€â”€â”¬â”€â”€â†’ Cls Conv â”€â”€â†’ Cls Pred    â”‚
â”‚                                  â”‚    (3Ã—3Ã—2)      (1Ã—1)       â”‚
â”‚                                  â”‚                             â”‚
â”‚                                  â””â”€â”€â†’ Reg Conv â”€â”€â”¬â”€â”€â†’ Reg Pred â”‚
â”‚                                       (3Ã—3Ã—2)    â”‚    (1Ã—1Ã—4)  â”‚
â”‚                                                  â”‚             â”‚
â”‚                                                  â””â”€â”€â†’ Obj Pred â”‚
â”‚                                                       (1Ã—1Ã—1)  â”‚
â”‚                                                                 â”‚
â”‚  Output: [Reg(4) + Obj(1) + Cls(num_classes)] per anchor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   POST-PROCESSING                              â”‚
â”‚                boxes.py (postprocess)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Confidence Thresholding                                    â”‚
â”‚  2. NMS (Non-Maximum Suppression)                              â”‚
â”‚  3. Coordinate Decoding                                        â”‚
â”‚  4. Final Detections                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### 1. Main Detector (`detector.py:18`)
```python
class YoloXDetector(th.nn.Module):
    def __init__(self, model_cfg):
        self.backbone = build_recurrent_backbone(backbone_cfg)
        self.fpn = build_yolox_fpn(fpn_cfg, in_channels)
        self.yolox_head = build_yolox_head(head_cfg, in_channels, strides)
```

#### 2. Recurrent Backbone (`maxvit_rnn.py:23`)
```python
class RNNDetector(BaseDetector):
    # 4-stage hierarchical backbone
    # Each stage: Downsample â†’ MaxViT Attention â†’ ConvLSTM
    # Strides: [4, 8, 16, 32]
    # Dims: [64, 128, 256, 512] (configurable)
```

#### 3. Feature Pyramid (`yolo_pafpn.py:18`)
```python
class YOLOPAFPN(nn.Module):
    # Top-down + Bottom-up feature fusion
    # Input: P2, P3, P4 from backbone
    # Output: N3, N4, N5 for detection
```

#### 4. Detection Head (`yolo_head.py:21`)
```python
class YOLOXHead(nn.Module):
    # Per-scale processing: 3 scales (8, 16, 32)
    # Outputs: Classification + Regression + Objectness
    # Uses anchor-free detection with center-based assignment
```

### Data Flow Summary

1. **Input**: Event representation (stacked histograms) - `(B, C, H, W)`
2. **Backbone**: 4-stage processing with recurrent memory - `{P1, P2, P3, P4}`
3. **FPN**: Multi-scale feature fusion - `{N3, N4, N5}`
4. **Detection**: Per-scale predictions - `[reg, obj, cls]`
5. **Output**: Bounding boxes with confidence scores

### Key Configuration Files

- **Model Config**: `config/model/maxvit_yolox/default.yaml`
- **Training Config**: `config/experiment/gen4/default.yaml`
- **Dataset Config**: `config/dataset/gen4.yaml`

### Memory and Recurrence

The key innovation of RVT is the **recurrent processing** at each backbone stage:

```python
# From maxvit_rnn.py:180
h_c_tuple = self.lstm(x, h_and_c_previous)  # ConvLSTM with memory
```

Each stage maintains **hidden states** across time steps, enabling temporal modeling of event sequences. This is managed by:

- **RNNStates** (`modules/detection.py:22`): Manages LSTM states per training mode
- **LstmStates** type: List of hidden/cell state tuples for each backbone stage

### âœ… PHASE 1 COMPLETED - P1 Feature Integration (2025-07-04)

**Successfully implemented 4-scale FPN for small object detection!**

**Modified Files:**
1. **`rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py`**:
   - Extended YOLOPAFPN class to support both 3-scale and 4-scale configurations
   - Added adaptive layer creation based on `num_scales` parameter
   - Implemented 4-scale top-down pathway: P4â†’P3â†’P2â†’P1
   - Implemented 4-scale bottom-up pathway: N1â†’N2â†’N3â†’N4
   - Maintains backward compatibility with original 3-scale mode

2. **`rvt_eTram/config/model/maxvit_yolox/default.yaml`**:
   - Changed `in_stages: [2, 3, 4]` â†’ `in_stages: [1, 2, 3, 4]`
   - Enables P1 features (stride 4) for small object detection

**Current Architecture Status:**
- âœ… **P1 features now utilized** (stride 4) - highest resolution features active
- âœ… **FPN supports 4 scales** (stride 4, 8, 16, 32) - enhanced fine detail
- âœ… **Detection head automatically supports 4 scales** - no changes needed
- ğŸ”„ **Anchor assignment** still optimized for medium/large objects (Phase 2)
- ğŸ”„ **Loss weighting** doesn't prioritize small object scales (Phase 2)

## Small Object Detection Enhancement Plan

### Strategy Overview

Event-based small object detection faces unique challenges:
- **Sparse Event Generation**: Small objects generate fewer events
- **Temporal Inconsistency**: Irregular movement patterns
- **Signal-to-Noise Ratio**: Difficulty distinguishing small objects from noise
- **Resolution Loss**: Current architecture discards high-resolution features

### âœ… Phase 1: High-Resolution Feature Integration (COMPLETED)

**Implementation Status: DONE âœ…**

#### 1.1 Backbone Modification âœ…
- **Status**: No changes needed - backbone already outputs all stages
- **Verification**: `maxvit_rnn.py:104` already returns `{1: P1, 2: P2, 3: P3, 4: P4}`

#### 1.2 FPN Extension âœ… 
- **File**: `rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py`
- **Changes**:
  - Extended `__init__` to support 4-scale: `in_stages=[1,2,3,4], in_channels=[64,128,256,512]`
  - Added conditional layer creation for 4-scale vs 3-scale
  - Implemented P1 processing layers: `reduce_conv2`, `C3_p2`, `bu_conv3`, `C3_n2`
  - Updated `forward()` method for 4-scale pathway

#### 1.3 Detection Head âœ…
- **Status**: No changes needed - YOLOXHead automatically supports variable scales
- **Verification**: Tested with 4 input scales, works correctly

**Testing Results:**
```
âœ“ 4-scale FPN initialization successful
âœ“ Forward pass successful - Output shapes:
  N1: torch.Size([2, 64, 80, 80]) (stride 4)
  N2: torch.Size([2, 128, 40, 40]) (stride 8)  
  N3: torch.Size([2, 256, 20, 20]) (stride 16)
  N4: torch.Size([2, 512, 10, 10]) (stride 32)
âœ“ Total detection scales: 4 (including stride 4 for small objects)
```

### Phase 2: Small Object Specialized Components (Medium Priority)

#### 2.1 Small Object Attention Module
```python
class SmallObjectAttention(nn.Module):
    def __init__(self, channels, scale_factor):
        self.motion_attention = MotionAwareAttention()
        self.spatial_attention = SpatialAttention()  
        self.scale_attention = ScaleAwareAttention(scale_factor)
```

#### 2.2 Temporal Feature Enhancement
```python
class TemporalFeatureEnhancer(nn.Module):
    def __init__(self):
        self.multi_temporal_fusion = MultiTemporalFusion()
        self.temporal_consistency = TemporalConsistency()
```

#### 2.3 Size-Aware Loss Function
```python
class SizeAwareLoss(nn.Module):
    def forward(self, pred, target, bbox_sizes):
        # Higher weight for smaller objects
        size_weights = torch.exp(-bbox_sizes / threshold)
        weighted_loss = base_loss * size_weights
```

### Phase 3: Training Strategy Optimization

#### 3.1 Adaptive Sampling
- Increase sampling ratio for small object samples
- Hard negative mining for difficult small object cases

#### 3.2 Multi-Scale Training  
- Various input resolutions during training
- Scale jittering for robustness improvement

### Implementation Priority

**High Priority (Immediate Implementation):**
1. P1 Feature Integration (Backbone + FPN + Head)
2. Size-Aware Loss Function
3. Training Strategy Improvements

**Medium Priority (Secondary Implementation):**
4. Small Object Attention Mechanisms
5. Temporal Enhancement Modules
6. Advanced Event Representations

**Low Priority (Experimental):**
7. Deformable Convolutions
8. Neural Architecture Search adaptations

### Expected Performance Improvements

- **Small Object AP**: +15-25% improvement expected
- **Overall mAP**: +5-10% improvement expected  
- **Temporal Consistency**: Significant improvement in tracking
- **False Positive Rate**: Reduction through better noise distinction

### File Modification Summary

**Core Files to Modify:**
- `rvt_eTram/models/detection/recurrent_backbone/maxvit_rnn.py` - Add P1 output
- `rvt_eTram/models/detection/yolox_extension/models/yolo_pafpn.py` - 4-scale FPN
- `rvt_eTram/models/detection/yolox/models/yolo_head.py` - 4-scale detection
- `rvt_eTram/models/detection/yolox/models/losses.py` - Size-aware loss
- `rvt_eTram/config/model/maxvit_yolox/default.yaml` - Configuration updates

**âœ… Configuration Changes Applied:**
```yaml
# rvt_eTram/config/model/maxvit_yolox/default.yaml
fpn:
  in_stages: [1, 2, 3, 4]  # âœ… IMPLEMENTED - Include P1 for small objects
  # Note: in_channels automatically inferred from backbone: [64, 128, 256, 512]

# Detection head automatically inherits strides: [4, 8, 16, 32] âœ… WORKING
```

**ğŸš€ Ready for Training:**
The implementation is complete and tested. You can now run training with enhanced small object detection:

```bash
# Train with P1 features enabled
python train.py model=rnndet dataset=gen4 dataset.path=<DATA_DIR> \
  +experiment/gen4="default.yaml" hardware.gpus=0 batch_size.train=6 \
  batch_size.eval=2 training.max_epochs=20
```

This systematic approach will significantly enhance small object detection performance while maintaining the RVT architecture's temporal modeling strengths.

## Small Object Detection Enhancement Experiments

### ğŸ¯ ì‹¤í—˜ ëª©í‘œ ë° ì„¤ì •

**ë°ì´í„°ì…‹ (ê³ ì •):**
- **ë©”ì¸ ë°ì´í„°ì…‹**: `etram_cls8_sample`
- **ê²½ë¡œ**: `/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample`
- **í´ë˜ìŠ¤ ìˆ˜**: 8ê°œ (Car, Truck, Motorcycle, Bicycle, Pedestrian, Bus, Static, Other)
- **ëª©í‘œ**: Small object detection ì„±ëŠ¥ í–¥ìƒ (í´ë˜ìŠ¤ 2,3,4: Motorcycle, Bicycle, Pedestrian)

**í•„ìˆ˜ ì„¤ì •ê°’ (ë¡œì»¬ ë©”ëª¨ë¦¬):**
- **í´ë˜ìŠ¤ ìˆ˜**: `+model.head.num_classes=8` (í•„ìˆ˜!)
- **í›ˆë ¨ ìŠ¤í…**: `training.max_steps=100000` (í•„ìˆ˜!)
- **Screen ì‚¬ìš©**: ëª¨ë“  í›ˆë ¨/validationì—ì„œ í•„ìˆ˜
- **ë°ì´í„°ì…‹**: `dataset=gen4`
- **ë°ì´í„° ê²½ë¡œ**: `dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample`

### ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (3-scale FPN)

**ì „ì²´ ì„±ëŠ¥:**
- **Overall mAP**: 34.02%
- **AP50**: 67.03%
- **AP75**: 30.79%

**í¬ê¸°ë³„ ì„±ëŠ¥:**
- **ğŸ”´ Small objects**: 17.28% mAP (í´ë˜ìŠ¤ 2,3,4: Motorcycle, Bicycle, Pedestrian) âš ï¸ **ì£¼ìš” ê°œì„  íƒ€ê²Ÿ**
- **ğŸŸ¡ Medium objects**: 34.03% mAP (í´ë˜ìŠ¤ 0,1,5,6,7: Car, Truck, Bus, Static, Other)
- **ğŸŸ¢ Large objects**: 56.94% mAP (ë§¤ìš° í° ê°ì²´ë“¤)

### ğŸ“‹ ì‹¤í—˜ í•œ ì‚¬ì´í´ í‘œì¤€ í”„ë¡œì„¸ìŠ¤

#### Phase 1: ì‹¤í—˜ ì„¤ì • ë° ì¤€ë¹„ (10ë¶„)

```bash
# 1. ì‹¤í—˜ í´ë” ìƒì„±
EXPERIMENT_ID="4scale_enhanced_100k"  # í˜•ì‹: {architecture}_{modification}_{steps}
mkdir -p experiments/${EXPERIMENT_ID}/{checkpoints,confusion_matrices,training_logs,validation_results}

# 2. ëª¨ë¸ ì„¤ì • ë³€ê²½
# íŒŒì¼: config/model/maxvit_yolox/default.yaml
# 3-scale: in_stages: [2, 3, 4]
# 4-scale: in_stages: [1, 2, 3, 4]  # P1 features í™œì„±í™”

# 3. ì„¤ì • ë°±ì—…
cp config/model/maxvit_yolox/default.yaml experiments/${EXPERIMENT_ID}/model_config.yaml
```

#### Phase 2: í›ˆë ¨ ì‹¤í–‰ (5-6ì‹œê°„)

```bash
# Screenì—ì„œ í›ˆë ¨ (í•„ìˆ˜!)
screen -dmS ${EXPERIMENT_ID}
screen -S ${EXPERIMENT_ID} -p 0 -X stuff "cd /home/oeoiewt/eTraM/rvt_eTram\n"
screen -S ${EXPERIMENT_ID} -p 0 -X stuff "python train.py model=rnndet dataset=gen4 dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample +experiment/gen4='default.yaml' hardware.gpus=0 batch_size.train=6 batch_size.eval=2 hardware.num_workers.train=4 hardware.num_workers.eval=3 training.max_steps=100000 dataset.train.sampling=stream +model.head.num_classes=8 wandb.project_name=etram_enhanced wandb.group_name=${EXPERIMENT_ID}; echo 'Training completed! Press Enter to continue...'; read\n"
```

#### Phase 3: ê²°ê³¼ ìˆ˜ì§‘ ë° ì •ë¦¬ (30ë¶„)

```bash
# 1. ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
cp dummy/${WANDB_ID}/checkpoints/epoch=*-step=100000-*.ckpt experiments/${EXPERIMENT_ID}/checkpoints/final_model.ckpt

# 2. Confusion Matrix ì´ë™
mv confM/* experiments/${EXPERIMENT_ID}/confusion_matrices/

# 3. Screen ì„¸ì…˜ ì •ë¦¬
screen -r ${EXPERIMENT_ID}  # ì™„ë£Œ í™•ì¸
```

#### Phase 4: Validation ë° ìƒì„¸ ì§€í‘œ (10ë¶„)

```bash
# Screenì—ì„œ Validation ì‹¤í–‰ (í•„ìˆ˜!)
screen -dmS validation_${EXPERIMENT_ID}
screen -S validation_${EXPERIMENT_ID} -p 0 -X stuff "cd /home/oeoiewt/eTraM/rvt_eTram\n"
screen -S validation_${EXPERIMENT_ID} -p 0 -X stuff "python validation.py dataset=gen4 dataset.path=/home/oeoiewt/eTraM/rvt_eTram/data/etram_cls8_sample checkpoint=experiments/${EXPERIMENT_ID}/checkpoints/final_model.ckpt +experiment/gen4='default.yaml' hardware.gpus=0 batch_size.eval=8 +model.head.num_classes=8; echo 'Validation completed! Press Enter to continue...'; read\n"

# ê²°ê³¼ ì €ì¥
mkdir -p validation_results/${EXPERIMENT_ID}
screen -r validation_${EXPERIMENT_ID} -X hardcopy /tmp/validation_${EXPERIMENT_ID}.txt
cp /tmp/validation_${EXPERIMENT_ID}.txt validation_results/${EXPERIMENT_ID}/validation_output.log
```

#### Phase 5: ê²°ê³¼ ë¶„ì„ ë° ë¬¸ì„œí™” (20ë¶„)

```bash
# 1. ì„±ëŠ¥ ìš”ì•½ íŒŒì¼ ìƒì„±
# validation_results/${EXPERIMENT_ID}/metrics_summary.txt
# validation_results/${EXPERIMENT_ID}/evaluation_info.txt

# 2. ì‹¤í—˜ ê²°ê³¼ JSON ìƒì„±
# experiments/${EXPERIMENT_ID}/experiment_results.json
```

#### Phase 6: Git ê´€ë¦¬ ë° ë³´ì¡´ (10ë¶„)

```bash
# ì‹¤í—˜ ê²°ê³¼ ì»¤ë°‹
git add experiments/${EXPERIMENT_ID}/
git add validation_results/${EXPERIMENT_ID}/
git commit -m "feat: complete ${EXPERIMENT_ID} experiment

- Model: [êµ¬ì²´ì  ì•„í‚¤í…ì²˜ ì„¤ëª…]
- Performance: mAP X.XX% (+X.X% vs baseline)
- Small objects: X.XX% mAP (+X.X% improvement)
- Key findings: [ì£¼ìš” ë°œê²¬ì‚¬í•­]

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)
Co-Authored-By: Claude <noreply@anthropic.com>"

# Screen ì„¸ì…˜ ì •ë¦¬
screen -S ${EXPERIMENT_ID} -X quit
screen -S validation_${EXPERIMENT_ID} -X quit
```

### ğŸ¯ ì„±ëŠ¥ ê°œì„  ëª©í‘œ

**Small Object Detection ëª©í‘œ:**
- **í˜„ì¬ ë² ì´ìŠ¤ë¼ì¸**: 17.28% mAP (Small objects)
- **4-scale FPN ëª©í‘œ**: 20-22% mAP (+15-25% í–¥ìƒ)
- **ì „ì²´ ì„±ëŠ¥ ëª©í‘œ**: 36-38% mAP (+5-10% í–¥ìƒ)

**ì‹¤í—˜ ì‹œë¦¬ì¦ˆ:**
1. âœ… **3-scale Baseline** (ì™„ë£Œ): 34.02% mAP
2. ğŸ”„ **4-scale Enhanced**: P1 features ì¶”ê°€ (ì§„í–‰ ì¤‘)
3. ğŸ”® **Size-aware Loss**: Loss function ê°œì„ 
4. ğŸ”® **Attention Modules**: Small object ì „ìš© attention

### ğŸ“ í‘œì¤€ íŒŒì¼ êµ¬ì¡°

```
experiments/{EXPERIMENT_ID}/
â”œâ”€â”€ checkpoints/final_model.ckpt
â”œâ”€â”€ confusion_matrices/*.png
â”œâ”€â”€ model_config.yaml
â”œâ”€â”€ training_logs/
â”œâ”€â”€ validation_results/ â†’ validation_results/{EXPERIMENT_ID}/
â””â”€â”€ experiment_results.json

validation_results/{EXPERIMENT_ID}/
â”œâ”€â”€ validation_output.log
â”œâ”€â”€ metrics_summary.txt
â””â”€â”€ evaluation_info.txt
```

### ğŸ”¬ ì‹¤í—˜ ê´€ë¦¬ ì›ì¹™

1. **ì¬í˜„ì„±**: ëª¨ë“  ì„¤ì •ì„ Gitìœ¼ë¡œ ê´€ë¦¬
2. **ì²´ê³„ì„±**: í‘œì¤€ í´ë” êµ¬ì¡° ìœ ì§€
3. **ë¹„êµì„±**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì •
4. **ë¬¸ì„œí™”**: ê° ì‹¤í—˜ì˜ ëª©ì ê³¼ ê²°ê³¼ ëª…í™•íˆ ê¸°ë¡

## 640Ã—360 í•´ìƒë„ ì†Œí˜• ê°ì²´ ê²€ì¶œ í˜ì‹  ì „ëµ

### ğŸ¯ í˜„ì¬ ì„±ëŠ¥ í•œê³„ ë° ëª©í‘œ

**í˜„ì¬ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥:**
- **Overall mAP**: 34.02%
- **Small objects mAP**: 17.28% (í´ë˜ìŠ¤ 2,3,4: Motorcycle, Bicycle, Pedestrian)

**ëª©í‘œ ì„±ëŠ¥:**
- **Small objects mAP**: 20-25% (+15-45% í–¥ìƒ)
- **Overall mAP**: 37-39% (+5-10% í–¥ìƒ)

### ğŸ“Š í˜ì‹ ì  ì ‘ê·¼ë²• ìš°ì„ ìˆœìœ„

| ë°©ë²• | ì˜ˆìƒ ê°œì„ í­ | êµ¬í˜„ ë‚œì´ë„ | ìš°ì„ ìˆœìœ„ |
|------|-------------|-------------|----------|
| **ConvLSTM + Temporal Attention** | +4-6% mAP | ì¤‘ê°„ | ğŸ”¥ ìµœê³  |
| **Size-aware Loss v2** | +3-5% mAP | ë‚®ìŒ | ğŸ”¥ ìµœê³  |
| **4-scale P1 ìµœì í™”** | +2-3% mAP | ë‚®ìŒ | âš¡ ë†’ìŒ |
| **Deformable Conv + SE** | +2-4% mAP | ì¤‘ê°„ | âš¡ ë†’ìŒ |
| **VTEI + Advanced Aug** | +1-3% mAP | ë†’ìŒ | ğŸ¯ ì¤‘ê°„ |
| **Multi-res Training** | +3-5% mAP | ë†’ìŒ | ğŸ¯ ì¤‘ê°„ |

### ğŸš€ 1ë‹¨ê³„: ê³ ê¸‰ ì‹œê°„ì  ëª¨ë¸ë§ (ì´ë²¤íŠ¸ ì¹´ë©”ë¼ íŠ¹í™”)

**ì´ë¡ ì  ê·¼ê±°**: ì´ë²¤íŠ¸ ì¹´ë©”ë¼ì˜ ì‹œê°„ì  ì •ë³´ëŠ” ì†Œí˜• ê°ì²´ì˜ motion patternì—ì„œ í•µì‹¬ì  ì—­í• 

#### A) ConvLSTM ê°•í™” (Recurrent YOLOv8 ê¸°ë°˜)
```python
class EnhancedConvLSTM(nn.Module):
    def __init__(self, input_channels, hidden_channels, num_layers=2):
        # Multi-temporal fusionìœ¼ë¡œ ì—¬ëŸ¬ ì‹œê°„ ìŠ¤ì¼€ì¼ í†µí•©
        self.multi_temporal_fusion = MultiTemporalFusion()
        self.conv_lstm = ConvLSTM(input_channels, hidden_channels, num_layers)
```

#### B) Sparse Cross-Attention (ASTMNet ê¸°ë°˜)
```python
class EventSparseAttention(nn.Module):
    def __init__(self, channels):
        # Event featuresì™€ backbone features ê°„ cross-attention
        self.cross_attention = SparseMultiHeadAttention()
        self.temporal_consistency = TemporalConsistencyModule()
```

#### C) Motion-Aware Feature Enhancement
```python
class MotionAwareEnhancer(nn.Module):
    def __init__(self):
        # Event polarity ê¸°ë°˜ motion direction ì˜ˆì¸¡
        self.motion_predictor = MotionPredictor()
        self.trajectory_tracker = TrajectoryTracker()
```

### âš–ï¸ 2ë‹¨ê³„: ì ì‘ì  Loss í•¨ìˆ˜ í˜ì‹ 

#### A) Size-Weighted Loss with Feedback
```python
class AdaptiveSizeAwareLoss(nn.Module):
    def forward(self, pred, target, bbox_sizes):
        # ë™ì  ê°€ì¤‘ì¹˜ with feedback mechanism
        feedback_multiplier = self.compute_feedback(training_history)
        small_weight = torch.exp(-bbox_sizes / threshold) * feedback_multiplier
        return weighted_loss
```

#### B) Temporal Consistency Loss
```python
class TemporalConsistencyLoss(nn.Module):
    def forward(self, current_pred, previous_pred, motion_vectors):
        # ì—°ì† í”„ë ˆì„ ê°„ small object tracking loss
        temporal_loss = self.consistency_penalty(current_pred, previous_pred)
        return temporal_loss
```

#### C) Hard Negative Mining for Small Objects
```python
class SmallObjectHardMining(nn.Module):
    def mine_hard_negatives(self, predictions, targets):
        # Small object ì£¼ë³€ì˜ ì–´ë ¤ìš´ negative samples ê°•í™” í•™ìŠµ
        hard_negatives = self.select_hard_samples(predictions, targets)
        return hard_negatives
```

### ğŸ” 3ë‹¨ê³„: Multi-Scale Feature í˜ì‹ 

#### A) 4-Scale FPN ìµœì í™”
```python
class OptimizedP1Features(nn.Module):
    def __init__(self):
        # P1 featuresë¥¼ small objects ì „ìš©ìœ¼ë¡œ fine-tuning
        self.small_object_enhancer = SmallObjectEnhancer()
        self.scale_specific_norm = ScaleSpecificNormalization()
```

#### B) Squeeze-and-Excitation + Deformable Convolutions
```python
class AdaptiveFeatureModule(nn.Module):
    def __init__(self, channels):
        # ê° scaleë³„ adaptive feature enhancement
        self.se_block = SEBlock(channels)
        self.deformable_conv = DeformableConv2d(channels, channels)
```

#### C) Adaptive Feature Fusion
```python
class EventDensityFusion(nn.Module):
    def __init__(self):
        # Event densityì— ë”°ë¥¸ dynamic feature fusion
        self.density_estimator = EventDensityEstimator()
        self.adaptive_fusion = AdaptiveFusionLayer()
```

### ğŸ“¡ 4ë‹¨ê³„: Event Data ì²˜ë¦¬ í˜ì‹ 

#### A) Volume of Ternary Event Images (VTEI)
```python
class VTEIRepresentation(nn.Module):
    def __init__(self):
        # Positive/Negative/Zero statesë¡œ ì„¸ë¶„í™”
        self.ternary_encoder = TernaryEventEncoder()
        self.volume_processor = VolumeProcessor()
```

#### B) Random Polarity Suppression
```python
class EventAugmentation(nn.Module):
    def __init__(self):
        # Small objectsì— íŠ¹í™”ëœ augmentation strategies
        self.polarity_suppression = PolaritySuppressionAug()
        self.noise_injection = NoiseInjectionAug()
```

#### C) Sparse Data Optimization
```python
class SparseEventProcessor(nn.Module):
    def __init__(self):
        # Memory-efficient sparse tensor operations
        self.sparse_conv = SparseConv3d()
        self.sparse_attention = SparseAttentionLayer()
```

### ğŸ—ï¸ 5ë‹¨ê³„: ì•„í‚¤í…ì²˜ ìˆ˜ì¤€ í˜ì‹ 

#### A) Multi-Resolution Training
```python
class MultiResolutionTraining:
    def __init__(self):
        # 640Ã—360 + 1280Ã—720 mixed training
        self.resolution_scheduler = ResolutionScheduler()
        self.scale_invariance_loss = ScaleInvarianceLoss()
```

#### B) Teacher-Student Distillation
```python
class SmallObjectDistillation(nn.Module):
    def __init__(self, teacher_model, student_model):
        # High-resolution teacher â†’ Low-resolution student
        self.knowledge_transfer = KnowledgeTransferModule()
        self.feature_distillation = FeatureDistillationLoss()
```

#### C) Neural Architecture Search (NAS)
```python
class EventNAS:
    def __init__(self):
        # Small object detectionì— íŠ¹í™”ëœ architecture ìë™ íƒìƒ‰
        self.search_space = EventBasedSearchSpace()
        self.performance_estimator = SmallObjectPerformanceEstimator()
```

### ğŸ¯ ì‹¤í—˜ ë¡œë“œë§µ

#### Phase 1: ì¦‰ì‹œ êµ¬í˜„ (1-2ì£¼)
1. **Size-aware Loss v2**: Dynamic feedback mechanism ì¶”ê°€
2. **4-scale P1 ìµœì í™”**: P1 features ì „ìš© ì²˜ë¦¬ ëª¨ë“ˆ
3. **ConvLSTM ê°•í™”**: Multi-temporal fusion êµ¬í˜„

#### Phase 2: ì¤‘ê¸° êµ¬í˜„ (2-4ì£¼)
1. **Temporal Attention**: Event-specific attention mechanisms
2. **Deformable Convolutions**: Shape-adaptive feature extraction
3. **Advanced Augmentation**: Small object íŠ¹í™” data augmentation

#### Phase 3: ì¥ê¸° ì‹¤í—˜ (4-8ì£¼)
1. **Multi-resolution Training**: Scale invariance ê°•í™”
2. **Teacher-Student Distillation**: Knowledge transfer
3. **Neural Architecture Search**: Optimal architecture íƒìƒ‰

### ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

**ëˆ„ì  ê°œì„  íš¨ê³¼:**
- **Phase 1 ì™„ë£Œ**: 17.28% â†’ 19-21% mAP (+10-20%)
- **Phase 2 ì™„ë£Œ**: 19-21% â†’ 22-24% mAP (+15-25%)  
- **Phase 3 ì™„ë£Œ**: 22-24% â†’ 25-27% mAP (+20-30%)

**ìµœì¢… ëª©í‘œ**: Small objects 25% mAP, Overall 38-40% mAP

## í‘œì¤€ ì‹¤í—˜ ë¬¸ì„œí™” í”„ë¡œì„¸ìŠ¤

### ğŸ“‹ í•„ìˆ˜ ë¬¸ì„œí™” íŒŒì¼ êµ¬ì¡°

```
experiments/{EXPERIMENT_ID}/
â”œâ”€â”€ experiment_hypothesis.txt      # ì‹¤í—˜ ê°€ì„¤ ë° ì´ë¡ ì  ê·¼ê±°
â”œâ”€â”€ modification_details.txt       # ì½”ë“œ ìˆ˜ì •ì‚¬í•­ ìƒì„¸ ê¸°ë¡  
â”œâ”€â”€ implementation_details.txt     # êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ ë° ì•„í‚¤í…ì²˜
â”œâ”€â”€ experiment_config.yaml         # ì‹¤í—˜ ì„¤ì • íŒŒì¼ ë°±ì—…
â”œâ”€â”€ code_changes_summary.txt       # ì£¼ìš” ë³€ê²½ì‚¬í•­ ìš”ì•½
â”œâ”€â”€ training_command.txt           # ì‹¤ì œ ì‚¬ìš©í•œ í›ˆë ¨ ëª…ë ¹ì–´
â”œâ”€â”€ memory_test_results.txt        # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ë° ìµœì í™” ê²°ê³¼
â”œâ”€â”€ experiment_results.json        # ìµœì¢… ì„±ëŠ¥ ê²°ê³¼ ë° ë¶„ì„
â”œâ”€â”€ checkpoints/final_model.ckpt   # ìµœì¢… í›ˆë ¨ëœ ëª¨ë¸
â”œâ”€â”€ confusion_matrices/            # Confusion matrix ì´ë¯¸ì§€ë“¤
â”œâ”€â”€ training_logs/                 # í›ˆë ¨ ë¡œê·¸ íŒŒì¼ë“¤
â””â”€â”€ validation_results/            # Validation ìƒì„¸ ê²°ê³¼
```

### ğŸ”„ ì‹¤í—˜ ë‹¨ê³„ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1. ì‹¤í—˜ ê¸°íš ë‹¨ê³„ (Day 0)
- [ ] **ì‹¤í—˜ ê°€ì„¤ ìˆ˜ë¦½**: `experiment_hypothesis.txt` ì‘ì„±
- [ ] **ì´ë¡ ì  ê·¼ê±° ì •ë¦¬**: ê¸°ì¡´ ì—°êµ¬ ë° ì˜ˆìƒ íš¨ê³¼ ë¶„ì„
- [ ] **ì„±ê³µ ê¸°ì¤€ ì„¤ì •**: ì •ëŸ‰ì  ì„±ëŠ¥ ëª©í‘œ ì„¤ì •
- [ ] **ë¦¬ìŠ¤í¬ ë¶„ì„**: ì ì¬ì  ì‹¤íŒ¨ ì›ì¸ ë° ëŒ€ì‘ì±…

#### 2. êµ¬í˜„ ë‹¨ê³„ (Day 1-3)
- [ ] **ì½”ë“œ ìˆ˜ì •ì‚¬í•­ ê¸°ë¡**: `modification_details.txt` ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- [ ] **ì„¤ì • íŒŒì¼ ë°±ì—…**: `experiment_config.yaml` ì €ì¥
- [ ] **Git ë¸Œëœì¹˜ ìƒì„±**: ì‹¤í—˜ ì „ìš© ë¸Œëœì¹˜ì—ì„œ ì‘ì—…
- [ ] **Unit Test ì‹¤í–‰**: ê°œë³„ ì»´í¬ë„ŒíŠ¸ ê¸°ëŠ¥ ê²€ì¦

#### 3. ë©”ëª¨ë¦¬ ìµœì í™” ë‹¨ê³„ (Day 3-4)
- [ ] **ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸**: `memory_test_results.txt` ì‘ì„±
- [ ] **Batch size ìµœì í™”**: OOM ë°©ì§€ ë° ì„±ëŠ¥ ê· í˜•
- [ ] **Hardware ì„¤ì • í™•ì¸**: GPU ë©”ëª¨ë¦¬ ë° Workers ìˆ˜ ì¡°ì •

#### 4. í›ˆë ¨ ì‹¤í–‰ ë‹¨ê³„ (Day 4-5)
- [ ] **Screen ì„¸ì…˜ ìƒì„±**: ì¥ì‹œê°„ í›ˆë ¨ ì•ˆì •ì„± í™•ë³´
- [ ] **í›ˆë ¨ ëª…ë ¹ì–´ ê¸°ë¡**: `training_command.txt` ì €ì¥
- [ ] **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: WandB ë˜ëŠ” ë¡œê·¸ë¥¼ í†µí•œ ì§„í–‰ìƒí™© ì¶”ì 
- [ ] **ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ í™•ì¸**: Overfitting ë° ìˆ˜ë ´ì„± ëª¨ë‹ˆí„°ë§

#### 5. ê²€ì¦ ë° ë¶„ì„ ë‹¨ê³„ (Day 5-6)
- [ ] **Validation ì‹¤í–‰**: í‘œì¤€ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
- [ ] **ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘**: mAP, AP50, AR ë“± ìƒì„¸ ë©”íŠ¸ë¦­
- [ ] **ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ**: ê¸°ì¡´ ì‹¤í—˜ ëŒ€ë¹„ ì„±ëŠ¥ ë³€í™” ë¶„ì„
- [ ] **Confusion Matrix ë¶„ì„**: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê°œì„ /ì €í•˜ í™•ì¸

#### 6. ë¬¸ì„œí™” ë° ì •ë¦¬ ë‹¨ê³„ (Day 6-7)
- [ ] **ì‹¤í—˜ ê²°ê³¼ ì¢…í•©**: `experiment_results.json` ì™„ì„±
- [ ] **ì£¼ìš” ë°œê²¬ì‚¬í•­ ì •ë¦¬**: ì„±ê³µ/ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
- [ ] **ë‹¤ìŒ ì‹¤í—˜ ë°©í–¥ ì œì‹œ**: ê°œì„ ì  ë° í›„ì† ì—°êµ¬ ê³„íš
- [ ] **Git ì»¤ë°‹**: ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¬¼ ì €ì¥

### ğŸ“Š ì„±ëŠ¥ í‰ê°€ í‘œì¤€

#### ì •ëŸ‰ì  ì§€í‘œ
- **Overall mAP**: ì „ì²´ í‰ê·  ì •ë°€ë„
- **Small Objects mAP**: í´ë˜ìŠ¤ 2,3,4 (Motorcycle, Bicycle, Pedestrian) ì„±ëŠ¥
- **AP50/AP75**: IoU thresholdë³„ ì„±ëŠ¥
- **AR (Average Recall)**: ì¬í˜„ìœ¨ ì§€í‘œ

#### ì •ì„±ì  ë¶„ì„
- **í›ˆë ¨ ì•ˆì •ì„±**: Loss ìˆ˜ë ´ íŒ¨í„´ ë° ì•ˆì •ì„±
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° í›ˆë ¨ ì†ë„
- **ì‹¤ìš©ì„±**: ì‹¤ì œ ë°°í¬ ê°€ëŠ¥ì„± ë° ê³„ì‚° ë³µì¡ë„

#### ë¹„êµ ê¸°ì¤€
- **ë² ì´ìŠ¤ë¼ì¸**: 3-scale baseline (34.02% mAP, 17.28% small objects)
- **ìƒëŒ€ ì„±ëŠ¥**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ /ì €í•˜ ì •ë„
- **ì‹¤í—˜ ì‹œë¦¬ì¦ˆ**: ë™ì¼ ì¹´í…Œê³ ë¦¬ ì‹¤í—˜ë“¤ ê°„ ë¹„êµ

### ğŸš¨ ì¼ë°˜ì  ì‹¤í—˜ ì‹¤íŒ¨ ì›ì¸ ë° ëŒ€ì‘

#### ë©”ëª¨ë¦¬ ê´€ë ¨ ì‹¤íŒ¨
- **ì›ì¸**: Batch size ê³¼ë‹¤, Workers ìˆ˜ ë¶€ì ì ˆ
- **ëŒ€ì‘**: ì²´ê³„ì  ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ë° ë‹¨ê³„ì  ì¡°ì •

#### ìˆ˜ë ´ ê´€ë ¨ ì‹¤íŒ¨  
- **ì›ì¸**: Learning rate ë¶€ì ì ˆ, Loss í•¨ìˆ˜ ë¶ˆê· í˜•
- **ëŒ€ì‘**: í›ˆë ¨ ì´ˆê¸° ìƒì„¸ ëª¨ë‹ˆí„°ë§ ë° ì¡°ê¸° ì¤‘ë‹¨

#### ì„±ëŠ¥ ì €í•˜
- **ì›ì¸**: ê³¼ë„í•œ ë³µì¡ì„±, ë°ì´í„°ì…‹ ë¯¸ìŠ¤ë§¤ì¹˜
- **ëŒ€ì‘**: ë‹¨ê³„ì  ë³µì¡ì„± ì¦ê°€ ë° Ablation study

#### ì¬í˜„ì„± ì‹¤íŒ¨
- **ì›ì¸**: ì„¤ì • íŒŒì¼ ë¶ˆì¼ì¹˜, Random seed ë¯¸ì„¤ì •
- **ëŒ€ì‘**: ì™„ì „í•œ ì„¤ì • ë°±ì—… ë° í™˜ê²½ ê³ ì •

## ì‹¤í—˜ ì„±ëŠ¥ ìˆœìœ„ ë° ì£¼ìš” ë°œê²¬ì‚¬í•­

### ğŸ“Š ì „ì²´ ì‹¤í—˜ ì„±ëŠ¥ ìˆœìœ„ (Overall mAP)

| ìˆœìœ„ | ì‹¤í—˜ëª… | Overall mAP | Small Objects mAP | ì•„í‚¤í…ì²˜ | ì£¼ìš” íŠ¹ì§• |
|------|--------|-------------|-------------------|----------|-----------|
| ğŸ¥‡ | **3scale_sizeaware_100k** | **34.08%** | 13.53% | 3-scale + Size-aware Loss | ì•ˆì •ì  ìµœê³  ì„±ëŠ¥ |
| ğŸ¥ˆ | **3scale_baseline** | **34.02%** | **17.28%** | 3-scale FPN | Small objects ìµœê³  |
| ğŸ¥‰ | **4scale_sizeaware_100k** | 32.23% | 12.75% | 4-scale + Size-aware | P1 features í™œìš© |
| 4 | **ABC_sod_basic_100k** | 31.7% | 14.8% | 4-scale + Multi-task | ABC ì ‘ê·¼ë²• |
| 5 | **patch2_4scale_sizeaware_200k** | 31.24% | 14.92% | patch=2 + 4-scale | Memory ì œì•½ |
| 6 | **4scale_enhanced_100k** | 30.93% | 14.83% | 4-scale FPN | P1 ë…¸ì´ì¦ˆ ë¬¸ì œ |
| 7 | **3scale_sizeaware_attention_100k** | 24.7% | TBD | 3-scale + Attention | ê·¹ì‹¬í•œ ì„±ëŠ¥ ì €í•˜ |

### ğŸ” Small Objects ì„±ëŠ¥ ìˆœìœ„

| ìˆœìœ„ | ì‹¤í—˜ëª… | Small Objects mAP | ë³€í™”ëŸ‰ | ìƒíƒœ |
|------|--------|------------------|--------|------|
| ğŸ¥‡ | **3scale_baseline** | **17.28%** | ê¸°ì¤€ì  | âœ… ìµœê³  ì„±ëŠ¥ |
| 2 | **ABC_sod_basic_100k** | 14.8% | -2.5% | âŒ í•˜ë½ |
| 3 | **patch2_4scale_sizeaware_200k** | 14.92% | -2.36% | âŒ í•˜ë½ |
| 4 | **4scale_enhanced_100k** | 14.83% | -2.45% | âŒ í•˜ë½ |
| 5 | **3scale_sizeaware_100k** | 13.53% | -3.75% | âŒ í•˜ë½ |
| 6 | **4scale_sizeaware_100k** | 12.75% | -4.53% | âŒ í•˜ë½ |

### ğŸš¨ í•µì‹¬ ë°œê²¬ì‚¬í•­

#### 1. **ë³µì¡ì„± ì—­ì„¤** (Complexity Paradox)
**ë°œê²¬**: ëª¨ë“  "ê°œì„ " ì‹œë„ê°€ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¼ìœ¼ì¼°ìŒ

**ê´€ì°°ëœ íŒ¨í„´**:
```
ë³µì¡ì„± ìˆœì„œ: 3scale_baseline < ABC < 4scale < attention
ì„±ëŠ¥ ìˆœì„œ:   3scale_baseline > ABC > 4scale > attention
```

**ê²°ë¡ **: 640Ã—360 í•´ìƒë„ì—ì„œ **ë‹¨ìˆœí•¨ì´ ìµœê³ ì˜ ì„±ëŠ¥**ì„ ë³´ì¥

#### 2. **í•´ìƒë„ ì œì•½ì˜ ê·¼ë³¸ì  í•œê³„**
**ë°œê²¬**: ì•„í‚¤í…ì²˜ ê°œì„ ë³´ë‹¤ **í•´ìƒë„ ì¦ê°€**ê°€ ë” ì¤‘ìš”

**ì¦ê±°**:
- P1 features (stride 4) í™œìš© ì‹œë„ë“¤ ëª¨ë‘ ì‹¤íŒ¨
- ê³ í•´ìƒë„ featuresì˜ ë…¸ì´ì¦ˆ ë¬¸ì œ
- Small objects ì •ë³´ ë¶€ì¡± (640Ã—360 ì œì•½)

**ì œì•ˆ**: 1280Ã—720 í•´ìƒë„ ìš°ì„  ì‹¤í—˜ í•„ìš”

#### 3. **Multi-task Learningì˜ í•œê³„**
**ABC ì‹¤í—˜ êµí›ˆ**:
- Multi-task learningì´ ë‹¨ì¼ taskë³´ë‹¤ ì–´ë ¤ì›€
- Gradient conflictsë¡œ ì¸í•œ ìµœì í™” ë¬¸ì œ
- Small datasetì—ì„œ ë³µì¡í•œ architectureì˜ ê³¼ì í•© ìœ„í—˜

#### 4. **Small Object Detectionì˜ ê·¼ë³¸ ë¬¸ì œ**
**ê´€ì°°**: ëª¨ë“  small object ê°œì„  ì‹œë„ ì‹¤íŒ¨

**ì›ì¸ ë¶„ì„**:
- **ë°ì´í„° í’ˆì§ˆ**: Event-based dataì˜ sparse nature
- **í•´ìƒë„ í•œê³„**: 640Ã—360ì—ì„œ small objects ì •ë³´ ë¶€ì¡±
- **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: Motorcycle(16K) vs Bicycle(1K) instances

### ğŸ¯ í˜ì‹ ì  ì ‘ê·¼ë²• ìš°ì„ ìˆœìœ„ (ì—…ë°ì´íŠ¸)

| ì ‘ê·¼ë²• | ê¸°ì¡´ ì˜ˆìƒ | ì‹¤ì œ ê²°ê³¼ | ìˆ˜ì •ëœ ìš°ì„ ìˆœìœ„ |
|--------|----------|----------|-----------------|
| **í•´ìƒë„ ì¦ê°€** | +3-5% mAP | **ë¯¸ì‹¤í—˜** | ğŸ”¥ **ìµœìš°ì„ ** |
| **Data-centric ê°œì„ ** | +1-3% mAP | **ë¯¸ì‹¤í—˜** | ğŸ”¥ **ìµœìš°ì„ ** |
| **Simple Loss ê°œì„ ** | +3-5% mAP | **ë¶€ë¶„ ì„±ê³µ** | âš¡ ë†’ìŒ |
| ~~Multi-task Learning~~ | +4-6% mAP | **-2.3% ì‹¤íŒ¨** | âŒ ë¹„ì¶”ì²œ |
| ~~4-scale FPN~~ | +2-3% mAP | **-3% ì‹¤íŒ¨** | âŒ ë¹„ì¶”ì²œ |
| ~~Attention Mechanisms~~ | +4-6% mAP | **-9.3% ì‹¤íŒ¨** | âŒ ì ˆëŒ€ ê¸ˆì§€ |

### ğŸ’¡ ì‹¤ìš©ì  ê¶Œì¥ì‚¬í•­

#### ì¦‰ì‹œ ì‹¤í–‰í•  ë°©í–¥
1. **í•´ìƒë„ ì¦ê°€**: 640Ã—360 â†’ 1280Ã—720 ì‹¤í—˜
2. **Data augmentation**: ì•„í‚¤í…ì²˜ ìˆ˜ì • ëŒ€ì‹  ë°ì´í„° ë‹¤ì–‘ì„±
3. **3-scale baseline ìµœì í™”**: ê²€ì¦ëœ ì•„í‚¤í…ì²˜ ì„¸ë°€ ì¡°ì •

#### í”¼í•´ì•¼ í•  ì ‘ê·¼ë²•
1. **ë³µì¡í•œ ì•„í‚¤í…ì²˜**: Attention, Multi-task learning
2. **P1 features í™œìš©**: ë…¸ì´ì¦ˆ ëŒ€ë¹„ ì´ë“ ì—†ìŒ
3. **ê³¼ë„í•œ engineering**: Simple is better

### ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ë¡œë“œë§µ (ìˆ˜ì •)

#### Phase 1: Resolution First (ìµœìš°ì„ )
- **ëª©í‘œ**: 1280Ã—720ì—ì„œ ë² ì´ìŠ¤ë¼ì¸ ì¬í˜„
- **ì˜ˆìƒ íš¨ê³¼**: Small objects 20-25% mAP ë‹¬ì„± ê°€ëŠ¥

#### Phase 2: Data-Centric (ê³ ìš°ì„ ìˆœìœ„)
- **ëª©í‘œ**: Advanced augmentation, ë°ì´í„° í’ˆì§ˆ ê°œì„ 
- **ì˜ˆìƒ íš¨ê³¼**: ì¶”ê°€ 2-3% mAP í–¥ìƒ

#### Phase 3: Focused Enhancement (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
- **ëª©í‘œ**: ê²€ì¦ëœ simple modificationsë§Œ
- **ì˜ˆìƒ íš¨ê³¼**: ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ 1-2% ì¶”ê°€ í–¥ìƒ

**ìµœì¢… ëª©í‘œ**: Small objects 25% mAP, Overall 40% mAP (ê³ í•´ìƒë„ì—ì„œ)

ì´ëŸ¬í•œ ë°œê²¬ì‚¬í•­ë“¤ì€ Event-based Small Object Detection ì—°êµ¬ì—ì„œ **"ë³µì¡í•¨ë³´ë‹¤ ê¸°ë³¸ì— ì¶©ì‹¤"**í•˜ë¼ëŠ” ì¤‘ìš”í•œ êµí›ˆì„ ì œê³µí•©ë‹ˆë‹¤.